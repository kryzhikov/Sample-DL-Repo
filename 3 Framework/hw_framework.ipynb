{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2DkIzfVdAxd"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BPQOD218dAxj"
   },
   "source": [
    "## Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBVpDMN6dAxs"
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Module(ABC):\n",
    "    def __init__(self):\n",
    "        self._train = True\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self,input, grad_output):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def parameters(self):\n",
    "        'Возвращает список собственных параметров.'\n",
    "        return []\n",
    "    \n",
    "    def grad_parameters(self):\n",
    "        'Возвращает список тензоров-градиентов для своих параметров.'\n",
    "        return []\n",
    "    \n",
    "    def train(self):\n",
    "        self._train = True\n",
    "    \n",
    "    def eval(self):\n",
    "        self._train = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TV9xeNAndAxv"
   },
   "source": [
    "# Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0O968yWrdAxx",
    "outputId": "a2d80f27-6958-4c48-e947-0f905db7cbb9"
   },
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__ (self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Проходит по все слоям.\"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "\n",
    "        self.output = input\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Backward — это как forward, только наоборот. (с)\"\"\"\n",
    "        \n",
    "        for i in range(len(self.layers)-1, 0, -1):\n",
    "            grad_output = self.layers[i].backward(self.layers[i-1].output, grad_output)\n",
    "        \n",
    "        grad_input = self.layers[0].backward(input, grad_output)\n",
    "        \n",
    "        return grad_input\n",
    "      \n",
    "    def parameters(self):\n",
    "        \"\"\"Конкантенирует параметры в один список.\"\"\"\n",
    "        res = []\n",
    "        for l in self.layers:\n",
    "            res += l.parameters()\n",
    "        return res\n",
    "    \n",
    "    def grad_parameters(self):\n",
    "        \"\"\"Конкантинирует градиенты в один список\"\"\"\n",
    "        res = []\n",
    "        for l in self.layers:\n",
    "            res += l.grad_parameters()\n",
    "        return res\n",
    "    \n",
    "    def train(self):\n",
    "        for layer in self.layers:\n",
    "            layer.train()\n",
    "    \n",
    "    def eval(self):\n",
    "        for layer in self.layers:\n",
    "            layer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50R6jFr8dAxy"
   },
   "source": [
    "# Слои"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guFtN_3FdAxz"
   },
   "source": [
    "Приступим к реализации содержательной части — самих слоев.\n",
    "\n",
    "На вход всех слоев будет подаваться матрица размера `batch_size` $\\times$ `n_features` (см. описание `forward`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jdBP7xZzdAxz"
   },
   "source": [
    "Начнем с основного: линейный слой, он же fully-conected.\n",
    "\n",
    "$$ Y = X W + b $$\n",
    "\n",
    "Правильнее его называть афинным: после матричного умножения добавляется вектор $b$.\n",
    "\n",
    "`forward` у него трививальный, а `backward` уже сложнее: нужно посчитать градиенты относительно трёх вещей:\n",
    "1. Входных данных. Автор добродушен и спалит вам ответ, а вам нужно его доказать: $\\nabla X = W^T (\\nabla Y)$.\n",
    "2. Матрица весов $W$. Тут нужно подумать, как каждый вес влияет на каждое выходное значение, и выразить ваши мысли линейной алгеброй.\n",
    "3. Вектор $b$. С ним всё будет просто.\n",
    "\n",
    "Не забудьте, что `grad_params` должен иметь такие же размерности, как и соответствующие параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4uEjKIasdAx3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\DL-Tinkoff\\3 Framework\\hw_framework.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/DL-Tinkoff/3%20Framework/hw_framework.ipynb#ch0000008?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mLinear\u001b[39;00m(Module):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/DL-Tinkoff/3%20Framework/hw_framework.ipynb#ch0000008?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, dim_in, dim_out, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/DL-Tinkoff/3%20Framework/hw_framework.ipynb#ch0000008?line=2'>3</a>\u001b[0m         \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Module' is not defined"
     ]
    }
   ],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, dim_in, dim_out, bias=True):\n",
    "        super().__init__()\n",
    "        self._bias = bias\n",
    "        \n",
    "        # Xavier initialization: инциализируем так,\n",
    "        # что если на вход идет N(0, 1)\n",
    "        # то и на выходе должно идти N(0, 1)\n",
    "        stdv = 1/np.sqrt(dim_in)\n",
    "        self.W = np.random.uniform(-stdv, stdv, size=(dim_in, dim_out))\n",
    "        if self._bias:\n",
    "            self.b = np.random.uniform(-stdv, stdv, size=dim_out)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.output = np.dot(input, self.W)\n",
    "        self.output += self.b if self._bias else 0\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        self.grad_W = np.dot(input.T, grad_output)\n",
    "        grad_input = np.dot(grad_output, self.W.T)\n",
    "        if self._bias:\n",
    "            self.grad_b = np.mean(grad_output, axis=0)\n",
    "        return grad_input\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.W, self.b] if self._bias else [self.W]\n",
    "    \n",
    "    def grad_parameters(self):\n",
    "        return [self.grad_W, self.grad_b] if self._bias else [self.grad_W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-jLyEmMEdAx7"
   },
   "source": [
    "## Функции активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0ci6studAx7"
   },
   "source": [
    "**ReLU** — одна из самых простых функций активации:\n",
    "\n",
    "$$\n",
    "ReLU(x)=\n",
    "\\begin{cases}\n",
    "x, & x > 0\\\\\n",
    "0, & x \\leq 0\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "`ReLU` это очень простой слой, поэтому автору не жалко её реализовать его за вас:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrayJEANdAx7"
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "         super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = np.maximum(input, 0)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.multiply(grad_output, input > 0)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUTaMFaWdAx8"
   },
   "source": [
    "У ReLU есть проблема — у него бесполезная нулевая производная при $x < 0$.\n",
    "\n",
    "[**Leaky Rectified Linear Unit**](http://en.wikipedia.org/wiki%2FRectifier_%28neural_networks%29%23Leaky_ReLUs) — это его модифицированная версия, имеющая в отрицательных координатах не нулевой градиент, а просто помноженный на маленькую константу `slope`.\n",
    "\n",
    "$$\n",
    "LeakyReLU_k(x)=\n",
    "\\begin{cases}\n",
    "x, & x > 0\\\\\n",
    "kx, & x \\leq 0\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "При `slope` = 0 он превращается в обычный `ReLU`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMWPFjxldAx8"
   },
   "outputs": [],
   "source": [
    "class LeakyReLU(Module):\n",
    "    def __init__(self, slope=0.03):\n",
    "        super().__init__()\n",
    "        self.slope = slope\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.output = (input > 0)*input + (input <= 0)*self.slope*input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        grad_input = np.multiply(grad_output, (input > 0) + (input <= 0)*self.slope)\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EorDi-TMdAx9"
   },
   "source": [
    "**Сигмоида** определяется формулой $\\sigma(x) = \\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "<img width='500px' src='https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/2000px-Sigmoid-function-2.svg.png'>\n",
    "\n",
    "Когда-то она была самой часто используемой функции активации, потому что имела логичную вероятностную интерпретацию (вероятность наличия какой-то фичи), но потом перестали, потому что на очень больших или маленьких значениях её производные почти нулевые (см. проблема затухающего градиента).\n",
    "\n",
    "Также используют [гипреболический тангенс](https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D0%BF%D0%B5%D1%80%D0%B1%D0%BE%D0%BB%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%B5_%D1%84%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D0%B8), который на самом деле просто сигмоида, отнормированная так, чтобы значения были в $[-1, 1]$: $tanh(x) = 2 \\sigma(x) - 1$. Мы его отдельно реализовывать не будем.\n",
    "\n",
    "Давайте посчитаем её производную:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sigma'(x) &= (\\frac{1}{1+e^{-x}})'\n",
    "\\\\         &= \\frac{e^{-x}}{(1+e^{-x})^2}\n",
    "\\\\         &= \\frac{1+e^{-x}-1}{(1+e^{-x})^2}\n",
    "\\\\         &= \\frac{1+e^{-x}}{(1+e^{-x})^2} - \\frac{1}{(1+e^{-x})^2}\n",
    "\\\\         &= \\frac{1}{1+e^{-x}} - \\frac{1}{(1+e^{-x})^2}\n",
    "\\\\         &= \\sigma(x) - \\sigma(x)^2\n",
    "\\\\         &= \\sigma(x)(1 - \\sigma(x))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LUag-tekdAx-"
   },
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.output = self.__class__._sigmoid(input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        sigma = self.__class__._sigmoid(input)\n",
    "        grad_input = np.multiply(grad_output, sigma*(1 - sigma))\n",
    "        return grad_input\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBhjcx0XdAx_"
   },
   "source": [
    "**Софтмакс** определяется так:\n",
    "\n",
    "$$ \\sigma(x)_k = \\frac{e^{x_k}}{\\sum_{i=1}^n e^{x_i} }$$\n",
    "\n",
    "Можно заметить, что сигмоида — это частный случай софтмакса. Его можно интерпретировать как вероятностное распределение: его выходы положительны и суммируются в единицу. Поэтому его используют как последний слой для классификации.\n",
    "\n",
    "Софтмакс — самый сложный с точки зрения написания `backward`. Как и все остальное, оно считается в 5 строчек кода, но [вывести их трудно](https://deepnotes.io/softmax-crossentropy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QS-6BIe6dAx_"
   },
   "outputs": [],
   "source": [
    "class SoftMax(Module):\n",
    "    def __init__(self):\n",
    "         super().__init__()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # важная деталь: если входы большие,\n",
    "        # то экспоненты будут ещё больше\n",
    "        self.output = self._softmax(input)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        p = self._softmax(input)\n",
    "        grad_input = p * (grad_output - (grad_output * p).sum(axis=1)[:, None])\n",
    "        return grad_input\n",
    "\n",
    "    def _softmax(self, x):\n",
    "        x = np.subtract(x, x.max(axis=1, keepdims=True))\n",
    "        e_m = np.exp(x)\n",
    "        sum_e = np.repeat(np.sum(e_m, axis=1), x.shape[-1]).reshape(*e_m.shape)\n",
    "        return e_m/sum_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-5W8O3zdAyA"
   },
   "source": [
    "## Регуляризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5bmlefZdAyA"
   },
   "source": [
    "Самый популярный регуляризатор в нейросетях — [**дропаут**](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf). Идея простая: просто помножим поэлементно входные данные на случайную бинарную маску того же размера, как и сами данные. Сгенерировать маску можно через `np.random.binomial`.\n",
    "\n",
    "Дропаута обычно хватает как единственного регуляризатора. Если вы заметите, что сеть оверфитится — просто добавьте его побольше.\n",
    "\n",
    "**У дропаута разное поведение в режимах `train` и `eval`**. При `eval` он не должен делать ничего, а в `train` вместо применения маски нужно ещё домножить вход на $p$, чтобы скомпенсировать дропаут при обучении (так математическое ожидание значений будет такое же, как на трейне)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nZsHZUDdAyA"
   },
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.p = p\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        if self._train:\n",
    "            p_save = 1 - self.p\n",
    "            self.mask = np.random.binomial(1, p=p_save, size=input.shape)/p_save\n",
    "            self.output = self.mask*input\n",
    "        else:\n",
    "            self.output = input\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        if self._train:\n",
    "            grad_input = self.mask*grad_output\n",
    "        else:\n",
    "            grad_input = grad_output\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0YNMOOIdAyB"
   },
   "source": [
    "`BatchNorm` -- относительно современный слой, сильно улучшающий сходимость. Всё, что он делает -- это нормирует свои входные значения так, что на выходе получаются значения со средним 0 и дисперсией 1.\n",
    "\n",
    "<img width='300px' src='https://wiseodd.github.io/img/2016-07-04-batchnorm/00.png'>\n",
    "\n",
    "Почитать про вывод градиента для него можно тут: https://wiseodd.github.io/techblog/2016/07/04/batchnorm/\n",
    "\n",
    "BatchNorm тоже по-разному ведёт себя при обучении и инференсе. Во время инференса он использует в качестве оценки среднего и дисперсии свои экспоненциально усреднённые исторические значения. Это связано с тем, что батч может быть маленьким, и оценки среднего и дисперсии будут неточными (при батче размера 1 дисперсия вообще будет нулевая, и нам в алгоритме нужно будет делить на ноль)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bkl3--6RdAyB"
   },
   "outputs": [],
   "source": [
    "class BatchNorm(Module):\n",
    "    def __init__(self, num_features, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.sigma_mean = 1\n",
    "        self.mu_mean = 0\n",
    "    \n",
    "    def forward(self, input):\n",
    "        if self._train:\n",
    "            assert input.shape[0] > 1, \"Batch size need to be >1\"\n",
    "            self._mu = np.mean(input, axis=0)\n",
    "            self._sigma = np.var(input, axis=0)\n",
    "            self.mu_mean = self.mu_mean*.9 + self._mu*.1 \n",
    "            self.sigma_mean = self.sigma_mean*.9 + self._sigma*.1\n",
    "            self._input_norm = self._normalize(input, self._mu, self._sigma)\n",
    "            self.output = self.gamma*self._input_norm + self.beta\n",
    "        else:\n",
    "            self._input_norm = self._normalize(input, self.mu_mean, self.sigma_mean)\n",
    "            self.output = self.gamma*self._input_norm + self.beta\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        if self._train:\n",
    "            m = input.shape[0]\n",
    "            input_minus_mu = (input - self._mu)\n",
    "            dinput_norm = grad_output * self.gamma\n",
    "            dsigma = np.sum(dinput_norm*input_minus_mu*(-.5)*self.std_inv**3, axis=0)\n",
    "            dmu = np.sum(dinput_norm * (-self.std_inv), axis=0) \\\n",
    "                  + dsigma * np.mean(-2 * input_minus_mu, axis=0)\n",
    "            \n",
    "            self.grad_gamma = np.sum(grad_output * self._input_norm, axis=0)\n",
    "            self.grad_beta = np.sum(grad_output, axis=0)\n",
    "            grad_input = dinput_norm*self.std_inv + dsigma*input_minus_mu/m + dmu/m\n",
    "        else:\n",
    "            self.grad_gamma = np.sum(grad_output * self._input_norm, axis=0)\n",
    "            self.grad_beta = np.sum(grad_output, axis=0)\n",
    "            grad_input = grad_output * self.gamma * self.std_inv\n",
    "        return grad_input\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "    def grad_parameters(self):\n",
    "        return [self.grad_gamma, self.grad_beta]\n",
    "\n",
    "    def _normalize(self, input, mu, sigma):\n",
    "        self.std_inv = 1/np.sqrt(sigma + self.eps)\n",
    "        return (input - mu)*self.std_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IO_okIAhdAyC"
   },
   "source": [
    "## Критерии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jZL4UjNdAyC"
   },
   "source": [
    "Критерии — это специальные функции, которые меряют качество, имея реальные данные и предсказанные. Все критерии возвращают скаляр — одно число, усреднённое значение метрики по всему батчу.\n",
    "\n",
    "По сути это тоже модули, но мы всё равно создадим для них отдельный класс, потому что у них нет `train` / `eval`, а `backward` не требует `grad_output` — эта вершина и так конечная в вычислительном графе. Также нам не понадобится сохранять для них `output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "COX7VXtUdAyC"
   },
   "outputs": [],
   "source": [
    "class Criterion(ABC):\n",
    "    def __call__(self, input, target):\n",
    "        return self.forward(input, target)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, input, target):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, input, target):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YF1c6q1adAyD"
   },
   "source": [
    "В качестве примера реализуем среднюю квадратичную ошибку (`MSE`).\n",
    "\n",
    "Обратите внимание, что в критериях мы делим итоговое число на размер батча — мы не хотим, чтобы функция потерь зависела от количества примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLZXaG3OdAyD"
   },
   "outputs": [],
   "source": [
    "class MSE(Criterion):\n",
    "    def forward(self, input, target):\n",
    "        batch_size = input.shape[0]\n",
    "        self.output = np.sum(np.power(input - target, 2)) / batch_size\n",
    "        return self.output\n",
    " \n",
    "    def backward(self, input, target):\n",
    "        self.grad_output  = (input - target) * 2 / input.shape[0]\n",
    "        return self.grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBCAlI7CdAyE"
   },
   "source": [
    "Ваша задача посложнее: вам нужно реализовать кроссэнтропию — это стандартная функция потерь для классификации. Тут можно почитать про вывод её градиентов, а также софтмакса: https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "Напоминаем интуицию за принципом максимального правдоподобия: мы максимизируем произведение предсказанных вероятностей реально случившихся событий $ L = \\prod p_i $.\n",
    "\n",
    "Произведение оптимизировать не очень удобно, и поэтому мы возьмём логарифм (любой, ведь все логарифмы отличаются в константу раз) и будем вместо него максимизировать сумму:\n",
    "\n",
    "$$ \\log L = \\log \\prod p_i = \\sum \\log p_i $$\n",
    "\n",
    "Эту штуку называют кроссэнтропией. Такое название пошло из теории информации, но нам пока знать это не надо.\n",
    "\n",
    "Для удобноства вместо чисел — от 0 до 9 — будем использовать вектора размера 10, где будет стоять единица в нужном месте (такое кодирование называется one-hot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ckg7SdddAyF"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy(Criterion):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, input, target): \n",
    "        # чтобы нигде не было взятий логарифма от нуля:\n",
    "        eps = 1e-9\n",
    "        input_clamp = np.clip(input, eps, 1 - eps)\n",
    "        self.output = (-np.log(input_clamp) * target).sum(axis=1)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, input, target):\n",
    "        eps = 1e-9\n",
    "        input_clamp = np.clip(input, eps, 1 - eps)\n",
    "        grad_input = -target * 1/input_clamp\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизаторы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def __call__(self, params, gradients):\n",
    "        for weights, gradient in zip(params, gradients):\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        self.lr = lr\n",
    "\n",
    "    def __call__(self, params, gradients):\n",
    "        for weights, gradient in zip(params, gradients):\n",
    "            weights -= self.lr * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum(Optimizer):\n",
    "    def __init__(self, lr=1e-3, momentum=.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self._u = []\n",
    "\n",
    "    def __call__(self, params, gradients):\n",
    "        if len(self._u) == 0:\n",
    "            self._u = [0]*len(params)\n",
    "        for i, (weights, gradient) in enumerate(zip(params, gradients)):\n",
    "            self._u[i] = self._u[i]*self.momentum + self.lr*gradient\n",
    "            weights -= self._u[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp(Optimizer):\n",
    "    def __init__(self, lr=1e-3, eps=1e-8, beta=.9):\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.beta = beta\n",
    "        self._g = []\n",
    "\n",
    "    def __call__(self, params, gradients):\n",
    "        if len(self._g) == 0:\n",
    "            self._g = [0]*len(params)\n",
    "        for i, (weights, gradient) in enumerate(zip(params, gradients)):\n",
    "            self._g[i] = self._g[i]*self.beta + gradient*gradient*(1-self.beta)\n",
    "            weights -= (self.lr*gradient)/np.sqrt(self._g[i] + self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, lr=1e-3, beta1=.9, beta2=.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self._m = []\n",
    "        self._u = []\n",
    "        self.t = 1\n",
    "\n",
    "    def __call__(self, params, gradients):\n",
    "        if (len(self._u) == 0) and (len(self._m) == 0):\n",
    "            self._u = [0]*len(params)\n",
    "            self._m = [0]*len(params)\n",
    "        for i, (weights, gradient) in enumerate(zip(params, gradients)):\n",
    "            self._m[i] = self.beta1*self._m[i] + (1-self.beta1)*gradient\n",
    "            self._u[i] = self.beta2*self._u[i] + (1-self.beta2)*gradient*gradient\n",
    "            hat_m = self._m[i]/(1-self.beta1**self.t)\n",
    "            hat_u = self._u[i]/(1-self.beta2**self.t)\n",
    "            weights -= (self.lr*hat_m)/(np.sqrt(hat_u) + self.eps)\n",
    "        self.t += 1\n",
    "\n",
    "    def reset_t(self):\n",
    "        self.t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nadam(Optimizer):\n",
    "    def __init__(self, lr=1e-3, beta1=.9, beta2=.999, eps=1e-8):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self._m = []\n",
    "        self._u = []\n",
    "        self.t = 1\n",
    "\n",
    "    def __call__(self, params, gradients):\n",
    "        if (len(self._u) == 0) and (len(self._m) == 0):\n",
    "            self._u = [0]*len(params)\n",
    "            self._m = [0]*len(params)\n",
    "        for i, (weights, gradient) in enumerate(zip(params, gradients)):\n",
    "            self._m[i] = self.beta1*self._m[i] + (1-self.beta1)*gradient\n",
    "            self._u[i] = self.beta2*self._u[i] + (1-self.beta2)*gradient*gradient\n",
    "            hat_m = self._m[i]/(1-self.beta1**self.t)\n",
    "            hat_u = self._u[i]/(1-self.beta2**self.t)\n",
    "            weights -= (self.lr \\\n",
    "                *(self.beta1*hat_m + ((1-self.beta1)*gradient)/(1-self.beta1**self.t))) \\\n",
    "                /(np.sqrt(hat_u) + self.eps)\n",
    "        self.t += 1\n",
    "\n",
    "    def reset_t(self):\n",
    "        self.t = 1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw_framework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
