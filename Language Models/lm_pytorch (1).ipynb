{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j4xr_tWSl2o"
      },
      "source": [
        "# Fun with language modelling\n",
        "\n",
        "Если вы пропустили лекцию, то посмотрите слайды к ней — они где-то есть. Также полезно почитать:\n",
        "\n",
        "* [Unreasonable effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy)\n",
        "* [Официальный пример от PyTorch](https://github.com/pytorch/examples/tree/master/word_language_model)\n",
        "\n",
        "Рекомендуется заранее всё прочитать, чтобы понять, что от вас хотят. При желании, можете переписать всё так, как подсказывает ваше сердце.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9hduozyxSl2t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfjjLEwoc-RL",
        "outputId": "9770bb5b-0f13-46a2-93c3-0a54d3d09ed9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFeJy7rXSl2u"
      },
      "source": [
        "## Препроцессинг (3 балла)\n",
        "\n",
        "Возьмите какие-нибудь сырые данные. Википедия, «Гарри Поттер», «Игра Престолов», тексты Монеточки, твиты Тинькова — что угодно.\n",
        "\n",
        "Для простоты будем делать char-level модель. Выкиньте из текстов все ненужные символы (можете оставить только алфавит и, пунктуацию). Сопоставьте всем различным символам свой номер. Удобно это хранить просто в питоновском словаре (`char2idx`). Для генерации вам потребуется ещё и обратный словарь (`idx2char`). Вы что-то такое должны были писать на вступительной — можете просто переиспользовать код оттуда.\n",
        "\n",
        "Заранее зарезервируйте айдишники под служебные символы: `<START>`, `<END>`, `<PAD>`, `<UNK>`.\n",
        "\n",
        "Клёво будет написать отдельный класс, который делает токенизацию и детокенизацию."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "real_man = '/content/drive/MyDrive/Zhirinovsky.txt' # сборник цитат великого и неподражаемого Владимира Вольфовича\n",
        "len(open(real_man).readlines())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLuLtdQqd17M",
        "outputId": "77c9a473-c5a7-4d7c-f98a-cb507dc0d9a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2253"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rxWoG7jFSl2v"
      },
      "outputs": [],
      "source": [
        "class Vocab:\n",
        "    def __init__(self, data):\n",
        "        self.char2idx = {}\n",
        "        self.idx2char = {}\n",
        "        idx = 0\n",
        "        for i in data:\n",
        "            for j in i:\n",
        "                if j not in self.char2idx.keys():\n",
        "                    self.char2idx[j] = idx\n",
        "                    self.idx2char[idx] = j\n",
        "                    idx += 1 \n",
        "        idx += 1\n",
        "        for c in ['<START>', '<UNK>', '<PAD>', '<EOS>']:\n",
        "            self.char2idx[c] = idx\n",
        "            self.idx2char[idx] = c\n",
        "            idx += 1\n",
        "        \n",
        "        \n",
        "    \n",
        "    def tokenize(self, sequence):\n",
        "        # выполните какой-то базовый препроцессинг\n",
        "        # например, оставьте только алфавит и пунктуацию\n",
        "        return [self.char2idx[char] for char in sequence]\n",
        "    \n",
        "    def detokenize(self, sequence):\n",
        "        return ''.join([self.idx2char[idx] for idx in sequence])\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.char2idx) + 1 # 0, 1, 2, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WADCKZuWSl2w"
      },
      "outputs": [],
      "source": [
        "class TextDataset:\n",
        "    \n",
        "    def __init__(self, data_path):\n",
        "        # обучите вокаб\n",
        "        \n",
        "        self.min_len = 40  # предложения с меньшим количеством символов не будут рассматриваться\n",
        "        self.max_len = 150 # предложения с большим количеством символов будут обрезаться\n",
        "\n",
        "        # разделите данные на отдельные сэмплы для обучения\n",
        "        # (просто список из сырых строк)\n",
        "        self.data = [line.lstrip()[6:-8].lower() for line in open(data_path) if (line.lstrip().startswith('<text>')\\\n",
        "                                                                         and self.min_len < len(line.lstrip()[6:-8]) < self.max_len - 1)]\n",
        "        self.vocab = Vocab(self.data)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "        sample = self.vocab.tokenize(sample)\n",
        "        sample.insert(0, self.vocab.char2idx['<START>'])\n",
        "        sample.insert(-1, self.vocab.char2idx['<EOS>'])\n",
        "        while len(sample) < self.max_len:\n",
        "            sample.append(self.vocab.char2idx['<PAD>'])\n",
        "        target = sample[1:]\n",
        "        target.append(self.vocab.char2idx['<EOS>'])\n",
        "        sample = torch.tensor(sample, dtype=torch.long)\n",
        "        target = torch.tensor(target, dtype=torch.long)\n",
        "        return sample, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTmO8wLwSl2x"
      },
      "source": [
        "Если у вас какой-то большой массив текста (скажем, статьи Википедии), вы можете просто нарезать из него кусочки фиксированной длины и так их подавать в модель.\n",
        "\n",
        "Если же вы хотите приключений, то можно разбить этот текст на предложения (`nltk.sent_tokenize`), и тогда все примеры будут разной длины. По соображениям производительности, вы не хотите использовать самые длинные и самые короткие сэмплы, поэтому имеет смысл обрезать их по длине."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8oFHxIWSl2z"
      },
      "source": [
        "Разобьём на обучение и валидацию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vngOVQMkSl2z"
      },
      "outputs": [],
      "source": [
        "dataset = TextDataset(real_man)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][0].size(), dataset[1][1].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWV4gi0YsWpk",
        "outputId": "776feb5b-b2b5-4982-a6e8-396e53359491"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([150]), torch.Size([150]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDb2xWSRSl20"
      },
      "source": [
        "## Модель (3 балла)\n",
        "\n",
        "Примерно такое должно зайти:\n",
        "\n",
        "* Эмбеддинг\n",
        "* LSTM / GRU\n",
        "* Линейный слой\n",
        "* Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RS15KjpTSl20"
      },
      "outputs": [],
      "source": [
        "class LM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, tie_weights):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        \n",
        "        if tie_weights:\n",
        "            # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
        "            # https://arxiv.org/abs/1608.05859\n",
        "            assert hidden_dim == embedding_dim\n",
        "            self.decoder.weight = self.encoder.weight\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.reshape(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.reshape(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # начальный хидден должен быть нулевой\n",
        "        # (либо хоть какой-то константный для всего обучения)\n",
        "        return torch.zeros(self.num_layers, batch_size, self.hidden_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ksav6UH4Sl21"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11PFBOs0Sl21",
        "outputId": "f1703727-c9ed-43b6-85f6-75d153c789b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ]
        }
      ],
      "source": [
        "epochs = 500\n",
        "lr = 1e-3\n",
        "batch_size = 64\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "model = LM(\n",
        "    vocab_size = len(dataset.vocab),\n",
        "    embedding_dim = 128,\n",
        "    hidden_dim = 128,\n",
        "    num_layers = 1,\n",
        "    dropout = 0.1,\n",
        "    tie_weights = True\n",
        ").to(device)\n",
        "\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.1) #torch.optim.Adam(model.parameters(), lr=lr)\n",
        "optimizer = torch.optim.NAdam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer)\n",
        "\n",
        "train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "test = DataLoader(test_set, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FyeJH_sPSl21",
        "outputId": "1006d52c-745b-4973-b51c-b73605357c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 0 | Mean training loss : 6.131 | Mean test loss : 5.022\n",
            "Epoch : 1 | Mean training loss : 5.268 | Mean test loss : 4.320\n",
            "Epoch : 2 | Mean training loss : 4.672 | Mean test loss : 3.765\n",
            "Epoch : 3 | Mean training loss : 4.133 | Mean test loss : 3.302\n",
            "Epoch : 4 | Mean training loss : 3.703 | Mean test loss : 2.926\n",
            "Epoch : 5 | Mean training loss : 3.343 | Mean test loss : 2.662\n",
            "Epoch : 6 | Mean training loss : 3.131 | Mean test loss : 2.395\n",
            "Epoch : 7 | Mean training loss : 2.904 | Mean test loss : 2.258\n",
            "Epoch : 8 | Mean training loss : 2.720 | Mean test loss : 2.133\n",
            "Epoch : 9 | Mean training loss : 2.617 | Mean test loss : 2.046\n",
            "Epoch : 10 | Mean training loss : 2.553 | Mean test loss : 1.978\n",
            "Epoch : 11 | Mean training loss : 2.468 | Mean test loss : 1.904\n",
            "Epoch : 12 | Mean training loss : 2.406 | Mean test loss : 1.894\n",
            "Epoch : 13 | Mean training loss : 2.324 | Mean test loss : 1.835\n",
            "Epoch : 14 | Mean training loss : 2.288 | Mean test loss : 1.833\n",
            "Epoch : 15 | Mean training loss : 2.275 | Mean test loss : 1.791\n",
            "Epoch : 16 | Mean training loss : 2.236 | Mean test loss : 1.788\n",
            "Epoch : 17 | Mean training loss : 2.196 | Mean test loss : 1.752\n",
            "Epoch : 18 | Mean training loss : 2.181 | Mean test loss : 1.767\n",
            "Epoch : 19 | Mean training loss : 2.152 | Mean test loss : 1.725\n",
            "Epoch : 20 | Mean training loss : 2.125 | Mean test loss : 1.738\n",
            "Epoch : 21 | Mean training loss : 2.114 | Mean test loss : 1.703\n",
            "Epoch : 22 | Mean training loss : 2.071 | Mean test loss : 1.722\n",
            "Epoch : 23 | Mean training loss : 2.067 | Mean test loss : 1.692\n",
            "Epoch : 24 | Mean training loss : 2.069 | Mean test loss : 1.697\n",
            "Epoch : 25 | Mean training loss : 2.045 | Mean test loss : 1.682\n",
            "Epoch : 26 | Mean training loss : 2.015 | Mean test loss : 1.691\n",
            "Epoch : 27 | Mean training loss : 2.005 | Mean test loss : 1.669\n",
            "Epoch : 28 | Mean training loss : 2.000 | Mean test loss : 1.676\n",
            "Epoch : 29 | Mean training loss : 1.981 | Mean test loss : 1.663\n",
            "Epoch : 30 | Mean training loss : 1.976 | Mean test loss : 1.669\n",
            "Epoch : 31 | Mean training loss : 1.971 | Mean test loss : 1.643\n",
            "Epoch : 32 | Mean training loss : 1.949 | Mean test loss : 1.663\n",
            "Epoch : 33 | Mean training loss : 1.968 | Mean test loss : 1.641\n",
            "Epoch : 34 | Mean training loss : 1.917 | Mean test loss : 1.652\n",
            "Epoch : 35 | Mean training loss : 1.927 | Mean test loss : 1.627\n",
            "Epoch : 36 | Mean training loss : 1.913 | Mean test loss : 1.646\n",
            "Epoch : 37 | Mean training loss : 1.897 | Mean test loss : 1.618\n",
            "Epoch : 38 | Mean training loss : 1.901 | Mean test loss : 1.636\n",
            "Epoch : 39 | Mean training loss : 1.886 | Mean test loss : 1.612\n",
            "Epoch : 40 | Mean training loss : 1.899 | Mean test loss : 1.631\n",
            "Epoch : 41 | Mean training loss : 1.870 | Mean test loss : 1.610\n",
            "Epoch : 42 | Mean training loss : 1.878 | Mean test loss : 1.621\n",
            "Epoch : 43 | Mean training loss : 1.888 | Mean test loss : 1.603\n",
            "Epoch : 44 | Mean training loss : 1.838 | Mean test loss : 1.615\n",
            "Epoch : 45 | Mean training loss : 1.841 | Mean test loss : 1.596\n",
            "Epoch : 46 | Mean training loss : 1.855 | Mean test loss : 1.610\n",
            "Epoch : 47 | Mean training loss : 1.834 | Mean test loss : 1.592\n",
            "Epoch : 48 | Mean training loss : 1.821 | Mean test loss : 1.601\n",
            "Epoch : 49 | Mean training loss : 1.813 | Mean test loss : 1.588\n",
            "Epoch : 50 | Mean training loss : 1.815 | Mean test loss : 1.595\n",
            "Epoch : 51 | Mean training loss : 1.816 | Mean test loss : 1.576\n",
            "Epoch : 52 | Mean training loss : 1.801 | Mean test loss : 1.607\n",
            "Epoch : 53 | Mean training loss : 1.809 | Mean test loss : 1.572\n",
            "Epoch : 54 | Mean training loss : 1.803 | Mean test loss : 1.608\n",
            "Epoch : 55 | Mean training loss : 1.770 | Mean test loss : 1.568\n",
            "Epoch : 56 | Mean training loss : 1.777 | Mean test loss : 1.591\n",
            "Epoch : 57 | Mean training loss : 1.768 | Mean test loss : 1.566\n",
            "Epoch : 58 | Mean training loss : 1.769 | Mean test loss : 1.584\n",
            "Epoch : 59 | Mean training loss : 1.757 | Mean test loss : 1.565\n",
            "Epoch : 60 | Mean training loss : 1.759 | Mean test loss : 1.579\n",
            "Epoch : 61 | Mean training loss : 1.748 | Mean test loss : 1.555\n",
            "Epoch : 62 | Mean training loss : 1.766 | Mean test loss : 1.577\n",
            "Epoch : 63 | Mean training loss : 1.748 | Mean test loss : 1.553\n",
            "Epoch : 64 | Mean training loss : 1.733 | Mean test loss : 1.578\n",
            "Epoch : 65 | Mean training loss : 1.760 | Mean test loss : 1.556\n",
            "Epoch : 66 | Mean training loss : 1.732 | Mean test loss : 1.566\n",
            "Epoch : 67 | Mean training loss : 1.730 | Mean test loss : 1.555\n",
            "Epoch : 68 | Mean training loss : 1.714 | Mean test loss : 1.559\n",
            "Epoch : 69 | Mean training loss : 1.713 | Mean test loss : 1.546\n",
            "Epoch : 70 | Mean training loss : 1.719 | Mean test loss : 1.560\n",
            "Epoch : 71 | Mean training loss : 1.716 | Mean test loss : 1.554\n",
            "Epoch : 72 | Mean training loss : 1.714 | Mean test loss : 1.549\n",
            "Epoch : 73 | Mean training loss : 1.710 | Mean test loss : 1.539\n",
            "Epoch : 74 | Mean training loss : 1.694 | Mean test loss : 1.562\n",
            "Epoch : 75 | Mean training loss : 1.701 | Mean test loss : 1.532\n",
            "Epoch : 76 | Mean training loss : 1.694 | Mean test loss : 1.551\n",
            "Epoch : 77 | Mean training loss : 1.686 | Mean test loss : 1.530\n",
            "Epoch : 78 | Mean training loss : 1.674 | Mean test loss : 1.548\n",
            "Epoch : 79 | Mean training loss : 1.679 | Mean test loss : 1.527\n",
            "Epoch : 80 | Mean training loss : 1.680 | Mean test loss : 1.544\n",
            "Epoch : 81 | Mean training loss : 1.654 | Mean test loss : 1.521\n",
            "Epoch : 82 | Mean training loss : 1.682 | Mean test loss : 1.543\n",
            "Epoch : 83 | Mean training loss : 1.665 | Mean test loss : 1.526\n",
            "Epoch : 84 | Mean training loss : 1.657 | Mean test loss : 1.532\n",
            "Epoch : 85 | Mean training loss : 1.662 | Mean test loss : 1.521\n",
            "Epoch : 86 | Mean training loss : 1.642 | Mean test loss : 1.535\n",
            "Epoch : 87 | Mean training loss : 1.652 | Mean test loss : 1.518\n",
            "Epoch : 88 | Mean training loss : 1.644 | Mean test loss : 1.533\n",
            "Epoch : 89 | Mean training loss : 1.636 | Mean test loss : 1.514\n",
            "Epoch : 90 | Mean training loss : 1.632 | Mean test loss : 1.534\n",
            "Epoch : 91 | Mean training loss : 1.633 | Mean test loss : 1.511\n",
            "Epoch : 92 | Mean training loss : 1.625 | Mean test loss : 1.526\n",
            "Epoch : 93 | Mean training loss : 1.631 | Mean test loss : 1.508\n",
            "Epoch : 94 | Mean training loss : 1.619 | Mean test loss : 1.523\n",
            "Epoch : 95 | Mean training loss : 1.623 | Mean test loss : 1.515\n",
            "Epoch : 96 | Mean training loss : 1.626 | Mean test loss : 1.517\n",
            "Epoch : 97 | Mean training loss : 1.604 | Mean test loss : 1.510\n",
            "Epoch : 98 | Mean training loss : 1.614 | Mean test loss : 1.512\n",
            "Epoch : 99 | Mean training loss : 1.614 | Mean test loss : 1.510\n",
            "Epoch : 100 | Mean training loss : 1.611 | Mean test loss : 1.511\n",
            "Epoch : 101 | Mean training loss : 1.610 | Mean test loss : 1.496\n",
            "Epoch : 102 | Mean training loss : 1.600 | Mean test loss : 1.529\n",
            "Epoch : 103 | Mean training loss : 1.602 | Mean test loss : 1.501\n",
            "Epoch : 104 | Mean training loss : 1.607 | Mean test loss : 1.517\n",
            "Epoch : 105 | Mean training loss : 1.596 | Mean test loss : 1.503\n",
            "Epoch : 106 | Mean training loss : 1.584 | Mean test loss : 1.514\n",
            "Epoch : 107 | Mean training loss : 1.602 | Mean test loss : 1.505\n",
            "Epoch : 108 | Mean training loss : 1.580 | Mean test loss : 1.512\n",
            "Epoch : 109 | Mean training loss : 1.587 | Mean test loss : 1.504\n",
            "Epoch : 110 | Mean training loss : 1.593 | Mean test loss : 1.501\n",
            "Epoch : 111 | Mean training loss : 1.573 | Mean test loss : 1.497\n",
            "Epoch : 112 | Mean training loss : 1.567 | Mean test loss : 1.503\n",
            "Epoch : 113 | Mean training loss : 1.571 | Mean test loss : 1.497\n",
            "Epoch : 114 | Mean training loss : 1.576 | Mean test loss : 1.499\n",
            "Epoch : 115 | Mean training loss : 1.566 | Mean test loss : 1.497\n",
            "Epoch : 116 | Mean training loss : 1.564 | Mean test loss : 1.502\n",
            "Epoch : 117 | Mean training loss : 1.572 | Mean test loss : 1.487\n",
            "Epoch : 118 | Mean training loss : 1.557 | Mean test loss : 1.493\n",
            "Epoch : 119 | Mean training loss : 1.562 | Mean test loss : 1.496\n",
            "Epoch : 120 | Mean training loss : 1.549 | Mean test loss : 1.484\n",
            "Epoch : 121 | Mean training loss : 1.537 | Mean test loss : 1.491\n",
            "Epoch : 122 | Mean training loss : 1.540 | Mean test loss : 1.478\n",
            "Epoch : 123 | Mean training loss : 1.531 | Mean test loss : 1.490\n",
            "Epoch : 124 | Mean training loss : 1.556 | Mean test loss : 1.478\n",
            "Epoch : 125 | Mean training loss : 1.528 | Mean test loss : 1.499\n",
            "Epoch : 126 | Mean training loss : 1.520 | Mean test loss : 1.479\n",
            "Epoch : 127 | Mean training loss : 1.532 | Mean test loss : 1.484\n",
            "Epoch : 128 | Mean training loss : 1.531 | Mean test loss : 1.471\n",
            "Epoch : 129 | Mean training loss : 1.522 | Mean test loss : 1.500\n",
            "Epoch : 130 | Mean training loss : 1.536 | Mean test loss : 1.472\n",
            "Epoch : 131 | Mean training loss : 1.527 | Mean test loss : 1.490\n",
            "Epoch : 132 | Mean training loss : 1.503 | Mean test loss : 1.470\n",
            "Epoch : 133 | Mean training loss : 1.522 | Mean test loss : 1.492\n",
            "Epoch : 134 | Mean training loss : 1.518 | Mean test loss : 1.469\n",
            "Epoch : 135 | Mean training loss : 1.530 | Mean test loss : 1.475\n",
            "Epoch : 136 | Mean training loss : 1.507 | Mean test loss : 1.478\n",
            "Epoch : 137 | Mean training loss : 1.509 | Mean test loss : 1.464\n",
            "Epoch : 138 | Mean training loss : 1.509 | Mean test loss : 1.481\n",
            "Epoch : 139 | Mean training loss : 1.498 | Mean test loss : 1.467\n",
            "Epoch : 140 | Mean training loss : 1.492 | Mean test loss : 1.482\n",
            "Epoch : 141 | Mean training loss : 1.498 | Mean test loss : 1.462\n",
            "Epoch : 142 | Mean training loss : 1.498 | Mean test loss : 1.474\n",
            "Epoch : 143 | Mean training loss : 1.500 | Mean test loss : 1.472\n",
            "Epoch : 144 | Mean training loss : 1.490 | Mean test loss : 1.471\n",
            "Epoch : 145 | Mean training loss : 1.501 | Mean test loss : 1.466\n",
            "Epoch : 146 | Mean training loss : 1.482 | Mean test loss : 1.466\n",
            "Epoch : 147 | Mean training loss : 1.483 | Mean test loss : 1.467\n",
            "Epoch : 148 | Mean training loss : 1.481 | Mean test loss : 1.473\n",
            "Epoch : 149 | Mean training loss : 1.471 | Mean test loss : 1.468\n",
            "Epoch : 150 | Mean training loss : 1.483 | Mean test loss : 1.461\n",
            "Epoch : 151 | Mean training loss : 1.480 | Mean test loss : 1.469\n",
            "Epoch : 152 | Mean training loss : 1.471 | Mean test loss : 1.471\n",
            "Epoch : 153 | Mean training loss : 1.470 | Mean test loss : 1.470\n",
            "Epoch : 154 | Mean training loss : 1.469 | Mean test loss : 1.467\n",
            "Epoch : 155 | Mean training loss : 1.464 | Mean test loss : 1.465\n",
            "Epoch : 156 | Mean training loss : 1.447 | Mean test loss : 1.473\n",
            "Epoch : 157 | Mean training loss : 1.458 | Mean test loss : 1.461\n",
            "Epoch : 158 | Mean training loss : 1.455 | Mean test loss : 1.465\n",
            "Epoch : 159 | Mean training loss : 1.450 | Mean test loss : 1.465\n",
            "Epoch : 160 | Mean training loss : 1.458 | Mean test loss : 1.458\n",
            "Epoch : 161 | Mean training loss : 1.456 | Mean test loss : 1.452\n",
            "Epoch : 162 | Mean training loss : 1.460 | Mean test loss : 1.469\n",
            "Epoch : 163 | Mean training loss : 1.446 | Mean test loss : 1.469\n",
            "Epoch : 164 | Mean training loss : 1.443 | Mean test loss : 1.458\n",
            "Epoch : 165 | Mean training loss : 1.443 | Mean test loss : 1.472\n",
            "Epoch : 166 | Mean training loss : 1.446 | Mean test loss : 1.455\n",
            "Epoch : 167 | Mean training loss : 1.435 | Mean test loss : 1.466\n",
            "Epoch : 168 | Mean training loss : 1.428 | Mean test loss : 1.455\n",
            "Epoch : 169 | Mean training loss : 1.420 | Mean test loss : 1.467\n",
            "Epoch : 170 | Mean training loss : 1.428 | Mean test loss : 1.449\n",
            "Epoch : 171 | Mean training loss : 1.417 | Mean test loss : 1.467\n",
            "Epoch : 172 | Mean training loss : 1.421 | Mean test loss : 1.446\n",
            "Epoch : 173 | Mean training loss : 1.419 | Mean test loss : 1.471\n",
            "Epoch : 174 | Mean training loss : 1.409 | Mean test loss : 1.448\n",
            "Epoch : 175 | Mean training loss : 1.406 | Mean test loss : 1.462\n",
            "Epoch : 176 | Mean training loss : 1.408 | Mean test loss : 1.449\n",
            "Epoch : 177 | Mean training loss : 1.405 | Mean test loss : 1.466\n",
            "Epoch : 178 | Mean training loss : 1.404 | Mean test loss : 1.448\n",
            "Epoch : 179 | Mean training loss : 1.399 | Mean test loss : 1.462\n",
            "Epoch : 180 | Mean training loss : 1.391 | Mean test loss : 1.444\n",
            "Epoch : 181 | Mean training loss : 1.388 | Mean test loss : 1.453\n",
            "Epoch : 182 | Mean training loss : 1.401 | Mean test loss : 1.460\n",
            "Epoch : 183 | Mean training loss : 1.400 | Mean test loss : 1.449\n",
            "Epoch : 184 | Mean training loss : 1.393 | Mean test loss : 1.454\n",
            "Epoch : 185 | Mean training loss : 1.401 | Mean test loss : 1.454\n",
            "Epoch : 186 | Mean training loss : 1.385 | Mean test loss : 1.461\n",
            "Epoch : 187 | Mean training loss : 1.384 | Mean test loss : 1.445\n",
            "Epoch : 188 | Mean training loss : 1.384 | Mean test loss : 1.457\n",
            "Epoch : 189 | Mean training loss : 1.387 | Mean test loss : 1.447\n",
            "Epoch : 190 | Mean training loss : 1.362 | Mean test loss : 1.444\n",
            "Epoch : 191 | Mean training loss : 1.367 | Mean test loss : 1.454\n",
            "Epoch : 192 | Mean training loss : 1.377 | Mean test loss : 1.444\n",
            "Epoch : 193 | Mean training loss : 1.369 | Mean test loss : 1.452\n",
            "Epoch : 194 | Mean training loss : 1.368 | Mean test loss : 1.448\n",
            "Epoch : 195 | Mean training loss : 1.360 | Mean test loss : 1.452\n",
            "Epoch : 196 | Mean training loss : 1.361 | Mean test loss : 1.444\n",
            "Epoch : 197 | Mean training loss : 1.366 | Mean test loss : 1.451\n",
            "Epoch : 198 | Mean training loss : 1.368 | Mean test loss : 1.442\n",
            "Epoch : 199 | Mean training loss : 1.353 | Mean test loss : 1.455\n",
            "Epoch : 200 | Mean training loss : 1.345 | Mean test loss : 1.463\n",
            "Epoch : 201 | Mean training loss : 1.362 | Mean test loss : 1.448\n",
            "Epoch : 202 | Mean training loss : 1.343 | Mean test loss : 1.479\n",
            "Epoch : 203 | Mean training loss : 1.347 | Mean test loss : 1.447\n",
            "Epoch : 204 | Mean training loss : 1.341 | Mean test loss : 1.469\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-909a34441158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 2. Прогоните данные через модель, получите предсказания на каждом токене\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-ad02684d825d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 943\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    944\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_loss = []\n",
        "test_loss = []\n",
        "for e in range(epochs):\n",
        "    mean_train = 0\n",
        "    for x, y in train:\n",
        "        model.train()\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        # 0. Распакуйте данные на нужное устройство\n",
        "        # 1. Инициилизируйте hidden\n",
        "        hidden = model.init_hidden(len(x))\n",
        "        # 2. Прогоните данные через модель, получите предсказания на каждом токене\n",
        "        output, _ = model(x, hidden)\n",
        "        output = output.view(-1, output.size()[-1])\n",
        "        y = y.view(-1)\n",
        "        loss = criterion(output, y)\n",
        "        loss.backward()\n",
        "        # 3. Посчитайте лосс (maxlen независимых классификаций) и сделайте backward()\n",
        "        # 4. Клипните градиенты -- у RNN-ок с этим часто бывают проблемы\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
        "        # 5. Залоггируйте лосс куда-нибудь\n",
        "        \n",
        "        train_loss.append(loss.item())\n",
        "        mean_train += loss.item()\n",
        "        optimizer.step()\n",
        "    \n",
        "    for x, y in test:\n",
        "        mean_test = 0\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            hidden = model.init_hidden(len(x))\n",
        "            output, _ = model(x, hidden)\n",
        "            output = output.view(-1, output.size()[-1])\n",
        "            y = y.view(-1)\n",
        "            loss = criterion(output, y)\n",
        "            test_loss.append(loss.item())\n",
        "            mean_test += loss.item()\n",
        "    print('Epoch : {} | Mean training loss : {:.3f} | Mean test loss : {:.3f}'.format(e, mean_train / len(train), mean_test / len(test)))\n",
        "    if e % 50 == 0:\n",
        "        lr_scheduler.step()\n",
        "         # сдесь нужно сделать то же самое, только без backward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_loss)\n",
        "plt.plot(test_loss)\n",
        "plt.xlabel('iter')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "IW9kW398BiFe",
        "outputId": "8131c553-62eb-4411-c1e8-602e0e58c346"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90RFHWzSl22"
      },
      "source": [
        "## Спеллчекер (3 балла)\n",
        "\n",
        "Из языковой модели можно сделать простенький спеллчекер: можно визуализировать лоссы на каждом символе (либо какой-нибудь другой показатель неуверенности).\n",
        "\n",
        "Бонус: можете усреднить перплексии по словам и выделять их, а не отдельные символы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Nyh6pzK3Sl22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c16cda17-d316-4eb0-ec28-3b74f057a5d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 255, 255)\">Налейте</span> <span style=\"background: rgb(255, 255, 255)\">мне</span> <span style=\"background: rgb(255, 155, 155)\">экспрессо</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 255, 255)\">Э</span><span style=\"background: rgb(255, 255, 255)\">т</span><span style=\"background: rgb(255, 255, 255)\">у</span><span style=\"background: rgb(255, 255, 255)\"> </span><span style=\"background: rgb(255, 255, 255)\">д</span><span style=\"background: rgb(255, 255, 255)\">о</span><span style=\"background: rgb(255, 255, 255)\">м</span><span style=\"background: rgb(255, 255, 255)\">а</span><span style=\"background: rgb(255, 255, 255)\">ш</span><span style=\"background: rgb(255, 255, 255)\">к</span><span style=\"background: rgb(255, 255, 255)\">у</span><span style=\"background: rgb(255, 255, 255)\"> </span><span style=\"background: rgb(255, 255, 255)\">н</span><span style=\"background: rgb(255, 255, 255)\">у</span><span style=\"background: rgb(255, 255, 255)\">ж</span><span style=\"background: rgb(255, 255, 255)\">н</span><span style=\"background: rgb(255, 255, 255)\">о</span><span style=\"background: rgb(255, 255, 255)\"> </span><span style=\"background: rgb(255, 255, 255)\">с</span><span style=\"background: rgb(255, 255, 255)\">д</span><span style=\"background: rgb(255, 255, 255)\">а</span><span style=\"background: rgb(255, 255, 255)\">т</span><span style=\"background: rgb(255, 255, 255)\">ь</span><span style=\"background: rgb(255, 255, 255)\"> </span><span style=\"background: rgb(255, 255, 255)\">в</span><span style=\"background: rgb(255, 205, 205)\">т</span><span style=\"background: rgb(255, 195, 195)\">е</span><span style=\"background: rgb(255, 185, 185)\">ч</span><span style=\"background: rgb(255, 255, 255)\">е</span><span style=\"background: rgb(255, 255, 255)\">н</span><span style=\"background: rgb(255, 255, 255)\">и</span><span style=\"background: rgb(255, 105, 105)\">и</span><span style=\"background: rgb(255, 255, 255)\"> </span><span style=\"background: rgb(255, 255, 255)\">д</span><span style=\"background: rgb(255, 255, 255)\">в</span><span style=\"background: rgb(255, 255, 255)\">у</span><span style=\"background: rgb(255, 255, 255)\">х</span><span style=\"background: rgb(255, 255, 255)\"> </span><span style=\"background: rgb(255, 255, 255)\">н</span><span style=\"background: rgb(255, 255, 255)\">е</span><span style=\"background: rgb(255, 255, 255)\">д</span><span style=\"background: rgb(255, 255, 255)\">е</span><span style=\"background: rgb(255, 255, 255)\">л</span><span style=\"background: rgb(255, 255, 255)\">ь</span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "def print_colored(sequence, intensities, delimeter=''):\n",
        "    html = delimeter.join([\n",
        "        f'<span style=\"background: rgb({255}, {255-x}, {255-x})\">{c}</span>'\n",
        "        for c, x in zip(sequence, intensities) \n",
        "    ])\n",
        "    display(HTML(html))\n",
        "\n",
        "print_colored('Налейте мне экспрессо'.split(), [0, 0, 100], ' ')\n",
        "\n",
        "sequence = 'Эту домашку нужно сдать втечении двух недель'\n",
        "intensities = [0]*len(sequence)\n",
        "intensities[25] = 50\n",
        "intensities[26] = 60\n",
        "intensities[27] = 70\n",
        "intensities[31] = 150\n",
        "print_colored(sequence, intensities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hPfOhVDySl23"
      },
      "outputs": [],
      "source": [
        "def spellcheck(sequence):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        string_seq = sequence\n",
        "        # векторизуйте sequence; паддинги делать не нужно\n",
        "        sequence = torch.tensor(dataset.vocab.tokenize(sequence.lower()), dtype=torch.long).view(1, -1)\n",
        "        \n",
        "        # прогоните модель и посчитайте лосс, но не усредняйте\n",
        "        # с losses можно что-нибудь сделать для визуализации; например, в какую-нибудь степень возвести\n",
        "        hidden = model.init_hidden(len(sequence))\n",
        "        out, _ = model(sequence, hidden)\n",
        "        losses = nn.CrossEntropyLoss(reduction='none')(out.view(-1, out.size()[-1]), sequence[0])\n",
        "        print_colored(string_seq, losses**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wnyZNi8oSl23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "26d660b3-25ab-479e-dec2-aa51bbcdb0c5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 123.4803466796875, 123.4803466796875)\">В</span><span style=\"background: rgb(255, 139.52008056640625, 139.52008056640625)\"> </span><span style=\"background: rgb(255, 102.69132995605469, 102.69132995605469)\">э</span><span style=\"background: rgb(255, 188.67092895507812, 188.67092895507812)\">т</span><span style=\"background: rgb(255, 215.47732543945312, 215.47732543945312)\">о</span><span style=\"background: rgb(255, 224.10833740234375, 224.10833740234375)\">м</span><span style=\"background: rgb(255, 208.62713623046875, 208.62713623046875)\"> </span><span style=\"background: rgb(255, 192.2518310546875, 192.2518310546875)\">п</span><span style=\"background: rgb(255, 204.76620483398438, 204.76620483398438)\">р</span><span style=\"background: rgb(255, 235.68075561523438, 235.68075561523438)\">е</span><span style=\"background: rgb(255, 197.81118774414062, 197.81118774414062)\">т</span><span style=\"background: rgb(255, 211.17837524414062, 211.17837524414062)\">л</span><span style=\"background: rgb(255, 200.49696350097656, 200.49696350097656)\">о</span><span style=\"background: rgb(255, 106.26055908203125, 106.26055908203125)\">ж</span><span style=\"background: rgb(255, 215.31787109375, 215.31787109375)\">е</span><span style=\"background: rgb(255, 250.8366241455078, 250.8366241455078)\">н</span><span style=\"background: rgb(255, 248.68666076660156, 248.68666076660156)\">и</span><span style=\"background: rgb(255, 223.4938201904297, 223.4938201904297)\">и</span><span style=\"background: rgb(255, 212.34121704101562, 212.34121704101562)\">и</span><span style=\"background: rgb(255, 217.06390380859375, 217.06390380859375)\"> </span><span style=\"background: rgb(255, 199.23667907714844, 199.23667907714844)\">о</span><span style=\"background: rgb(255, 120.577392578125, 120.577392578125)\">ч</span><span style=\"background: rgb(255, 232.912353515625, 232.912353515625)\">е</span><span style=\"background: rgb(255, 249.73342895507812, 249.73342895507812)\">н</span><span style=\"background: rgb(255, 143.9358367919922, 143.9358367919922)\"> </span><span style=\"background: rgb(255, 185.54541015625, 185.54541015625)\">м</span><span style=\"background: rgb(255, 165.9481201171875, 165.9481201171875)\">н</span><span style=\"background: rgb(255, 218.97479248046875, 218.97479248046875)\">о</span><span style=\"background: rgb(255, 138.15023803710938, 138.15023803710938)\">г</span><span style=\"background: rgb(255, 216.224365234375, 216.224365234375)\">о</span><span style=\"background: rgb(255, 189.55941772460938, 189.55941772460938)\"> </span><span style=\"background: rgb(255, 196.87283325195312, 196.87283325195312)\">о</span><span style=\"background: rgb(255, 127.72059631347656, 127.72059631347656)\">ч</span><span style=\"background: rgb(255, 231.44871520996094, 231.44871520996094)\">е</span><span style=\"background: rgb(255, 213.94729614257812, 213.94729614257812)\">п</span><span style=\"background: rgb(255, 190.78970336914062, 190.78970336914062)\">я</span><span style=\"background: rgb(255, 213.8153076171875, 213.8153076171875)\">т</span><span style=\"background: rgb(255, 178.41104125976562, 178.41104125976562)\">о</span><span style=\"background: rgb(255, 190.00274658203125, 190.00274658203125)\">к</span><span style=\"background: rgb(255, 246.16122436523438, 246.16122436523438)\">.</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 189.2189178466797, 189.2189178466797)\">З</span><span style=\"background: rgb(255, 174.75482177734375, 174.75482177734375)\">д</span><span style=\"background: rgb(255, 232.72471618652344, 232.72471618652344)\">е</span><span style=\"background: rgb(255, 200.41757202148438, 200.41757202148438)\">с</span><span style=\"background: rgb(255, 214.38095092773438, 214.38095092773438)\">ь</span><span style=\"background: rgb(255, 193.23788452148438, 193.23788452148438)\"> </span><span style=\"background: rgb(255, 194.43280029296875, 194.43280029296875)\">п</span><span style=\"background: rgb(255, 229.04849243164062, 229.04849243164062)\">о</span><span style=\"background: rgb(255, 174.45974731445312, 174.45974731445312)\">я</span><span style=\"background: rgb(255, 103.26783752441406, 103.26783752441406)\">в</span><span style=\"background: rgb(255, 238.61083984375, 238.61083984375)\">и</span><span style=\"background: rgb(255, 227.6066436767578, 227.6066436767578)\">л</span><span style=\"background: rgb(255, 188.3956298828125, 188.3956298828125)\">а</span><span style=\"background: rgb(255, 229.87088012695312, 229.87088012695312)\">с</span><span style=\"background: rgb(255, 209.86920166015625, 209.86920166015625)\">ь</span><span style=\"background: rgb(255, 190.0976104736328, 190.0976104736328)\"> </span><span style=\"background: rgb(255, 160.29017639160156, 160.29017639160156)\">л</span><span style=\"background: rgb(255, 189.97735595703125, 189.97735595703125)\">и</span><span style=\"background: rgb(255, 189.9207763671875, 189.9207763671875)\">ш</span><span style=\"background: rgb(255, 190.00833129882812, 190.00833129882812)\">н</span><span style=\"background: rgb(255, 156.18997192382812, 156.18997192382812)\">н</span><span style=\"background: rgb(255, 197.05929565429688, 197.05929565429688)\">я</span><span style=\"background: rgb(255, 168.6770477294922, 168.6770477294922)\">я</span><span style=\"background: rgb(255, 192.0350341796875, 192.0350341796875)\"> </span><span style=\"background: rgb(255, 193.95181274414062, 193.95181274414062)\">б</span><span style=\"background: rgb(255, 217.0202178955078, 217.0202178955078)\">у</span><span style=\"background: rgb(255, 166.02679443359375, 166.02679443359375)\">к</span><span style=\"background: rgb(255, 79.27000427246094, 79.27000427246094)\">в</span><span style=\"background: rgb(255, 165.21820068359375, 165.21820068359375)\">а</span><span style=\"background: rgb(255, 225.974365234375, 225.974365234375)\">.</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 123.4803466796875, 123.4803466796875)\">В</span><span style=\"background: rgb(255, 139.52008056640625, 139.52008056640625)\"> </span><span style=\"background: rgb(255, 102.69132995605469, 102.69132995605469)\">э</span><span style=\"background: rgb(255, 188.67092895507812, 188.67092895507812)\">т</span><span style=\"background: rgb(255, 215.47732543945312, 215.47732543945312)\">о</span><span style=\"background: rgb(255, 224.10833740234375, 224.10833740234375)\">м</span><span style=\"background: rgb(255, 208.62713623046875, 208.62713623046875)\"> </span><span style=\"background: rgb(255, 192.2518310546875, 192.2518310546875)\">п</span><span style=\"background: rgb(255, 204.76620483398438, 204.76620483398438)\">р</span><span style=\"background: rgb(255, 235.68075561523438, 235.68075561523438)\">е</span><span style=\"background: rgb(255, 205.33041381835938, 205.33041381835938)\">д</span><span style=\"background: rgb(255, 227.0386962890625, 227.0386962890625)\">л</span><span style=\"background: rgb(255, 186.91685485839844, 186.91685485839844)\">о</span><span style=\"background: rgb(255, 102.82760620117188, 102.82760620117188)\">ж</span><span style=\"background: rgb(255, 209.3968505859375, 209.3968505859375)\">е</span><span style=\"background: rgb(255, 251.77755737304688, 251.77755737304688)\">н</span><span style=\"background: rgb(255, 248.46066284179688, 248.46066284179688)\">и</span><span style=\"background: rgb(255, 225.69656372070312, 225.69656372070312)\">и</span><span style=\"background: rgb(255, 222.74972534179688, 222.74972534179688)\"> </span><span style=\"background: rgb(255, 121.58592224121094, 121.58592224121094)\">в</span><span style=\"background: rgb(255, 178.0225830078125, 178.0225830078125)\">с</span><span style=\"background: rgb(255, 222.95297241210938, 222.95297241210938)\">е</span><span style=\"background: rgb(255, 163.15859985351562, 163.15859985351562)\"> </span><span style=\"background: rgb(255, 196.67913818359375, 196.67913818359375)\">н</span><span style=\"background: rgb(255, 202.19529724121094, 202.19529724121094)\">о</span><span style=\"background: rgb(255, 207.30841064453125, 207.30841064453125)\">р</span><span style=\"background: rgb(255, 190.43167114257812, 190.43167114257812)\">м</span><span style=\"background: rgb(255, 194.1300048828125, 194.1300048828125)\">а</span><span style=\"background: rgb(255, 239.74761962890625, 239.74761962890625)\">л</span><span style=\"background: rgb(255, 217.92770385742188, 217.92770385742188)\">ь</span><span style=\"background: rgb(255, 227.7406005859375, 227.7406005859375)\">н</span><span style=\"background: rgb(255, 217.32693481445312, 217.32693481445312)\">о</span><span style=\"background: rgb(255, 245.31179809570312, 245.31179809570312)\">.</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 136.21060180664062, 136.21060180664062)\">Ч</span><span style=\"background: rgb(255, 206.69931030273438, 206.69931030273438)\">т</span><span style=\"background: rgb(255, 209.2669219970703, 209.2669219970703)\">о</span><span style=\"background: rgb(255, 231.6923828125, 231.6923828125)\">н</span><span style=\"background: rgb(255, 241.75682067871094, 241.75682067871094)\">и</span><span style=\"background: rgb(255, 197.99034118652344, 197.99034118652344)\">б</span><span style=\"background: rgb(255, 208.8629150390625, 208.8629150390625)\">у</span><span style=\"background: rgb(255, 198.47520446777344, 198.47520446777344)\">д</span><span style=\"background: rgb(255, 193.86734008789062, 193.86734008789062)\">ь</span><span style=\"background: rgb(255, 197.92727661132812, 197.92727661132812)\"> </span><span style=\"background: rgb(255, 168.10067749023438, 168.10067749023438)\">п</span><span style=\"background: rgb(255, 227.87730407714844, 227.87730407714844)\">и</span><span style=\"background: rgb(255, 87.06581115722656, 87.06581115722656)\">ш</span><span style=\"background: rgb(255, 232.8113555908203, 232.8113555908203)\">е</span><span style=\"background: rgb(255, 195.11318969726562, 195.11318969726562)\">т</span><span style=\"background: rgb(255, 195.5595703125, 195.5595703125)\">ь</span><span style=\"background: rgb(255, 185.68582153320312, 185.68582153320312)\">с</span><span style=\"background: rgb(255, 216.9328155517578, 216.9328155517578)\">я</span><span style=\"background: rgb(255, 213.7240447998047, 213.7240447998047)\"> </span><span style=\"background: rgb(255, 143.80191040039062, 143.80191040039062)\">ч</span><span style=\"background: rgb(255, 228.80604553222656, 228.80604553222656)\">е</span><span style=\"background: rgb(255, 220.83935546875, 220.83935546875)\">р</span><span style=\"background: rgb(255, 241.96090698242188, 241.96090698242188)\">и</span><span style=\"background: rgb(255, 194.7537841796875, 194.7537841796875)\">з</span><span style=\"background: rgb(255, 167.6295623779297, 167.6295623779297)\"> </span><span style=\"background: rgb(255, 198.5189971923828, 198.5189971923828)\">д</span><span style=\"background: rgb(255, 227.73265075683594, 227.73265075683594)\">е</span><span style=\"background: rgb(255, 60.33790588378906, 60.33790588378906)\">ф</span><span style=\"background: rgb(255, 221.1739501953125, 221.1739501953125)\">и</span><span style=\"background: rgb(255, 239.75364685058594, 239.75364685058594)\">с</span><span style=\"background: rgb(255, 250.28126525878906, 250.28126525878906)\">.</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span style=\"background: rgb(255, 222.51171875, 222.51171875)\">С</span><span style=\"background: rgb(255, 153.23977661132812, 153.23977661132812)\">л</span><span style=\"background: rgb(255, 184.5928955078125, 184.5928955078125)\">о</span><span style=\"background: rgb(255, 152.99847412109375, 152.99847412109375)\">в</span><span style=\"background: rgb(255, 189.78662109375, 189.78662109375)\">а</span><span style=\"background: rgb(255, 177.26486206054688, 177.26486206054688)\"> </span><span style=\"background: rgb(255, 179.726318359375, 179.726318359375)\">н</span><span style=\"background: rgb(255, 119.85511779785156, 119.85511779785156)\">р</span><span style=\"background: rgb(255, 219.59957885742188, 219.59957885742188)\">п</span><span style=\"background: rgb(255, 171.286865234375, 171.286865234375)\">д</span><span style=\"background: rgb(255, 181.13572692871094, 181.13572692871094)\">з</span><span style=\"background: rgb(255, 215.91342163085938, 215.91342163085938)\">х</span><span style=\"background: rgb(255, 156.9211883544922, 156.9211883544922)\"> </span><span style=\"background: rgb(255, 165.80657958984375, 165.80657958984375)\">н</span><span style=\"background: rgb(255, 238.51885986328125, 238.51885986328125)\">е</span><span style=\"background: rgb(255, 187.42230224609375, 187.42230224609375)\"> </span><span style=\"background: rgb(255, 224.53468322753906, 224.53468322753906)\">с</span><span style=\"background: rgb(255, 156.0850830078125, 156.0850830078125)\">у</span><span style=\"background: rgb(255, 146.19134521484375, 146.19134521484375)\">щ</span><span style=\"background: rgb(255, 249.29698181152344, 249.29698181152344)\">е</span><span style=\"background: rgb(255, 227.5415496826172, 227.5415496826172)\">с</span><span style=\"background: rgb(255, 136.449462890625, 136.449462890625)\">д</span><span style=\"background: rgb(255, 62.20796203613281, 62.20796203613281)\">в</span><span style=\"background: rgb(255, 176.97021484375, 176.97021484375)\">у</span><span style=\"background: rgb(255, 205.88108825683594, 205.88108825683594)\">е</span><span style=\"background: rgb(255, 206.32928466796875, 206.32928466796875)\">т</span><span style=\"background: rgb(255, 229.49258422851562, 229.49258422851562)\">.</span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "sequences = ['В этом претложениии очен много очепяток.', \n",
        "             'Здесь появилась лишнняя буква.', \n",
        "             'В этом предложении все нормально.', \n",
        "             'Чтонибудь пишеться чериз дефис.', \n",
        "             'Слова нрпдзх не сущесдвует.']\n",
        "\n",
        "for sequence in sequences:\n",
        "    spellcheck(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js82nszOSl23"
      },
      "source": [
        "## Генерация предложений (3 балла)\n",
        "\n",
        "* Поддерживайте hidden state при генерации. Не пересчитывайте ничего больше одного раза.\n",
        "* Прикрутите температуру: это когда при сэмплировании все логиты (то, что перед софтмаксом) делятся на какое-то число (по умолчанию 1, тогда ничего не меняется). Температура позволяет делать trade-off между разнообразием и правдоподобием (подробнее — см. блог Карпатого).\n",
        "* Ваша реализация должна уметь принимать строку seed — то, с чего должно начинаться сгенерированная строка."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(num_tokens, seed=\"\", temperature=1.0):\n",
        "    model.eval()\n",
        "    #print('hello ', seed)\n",
        "    input = torch.LongTensor(dataset.vocab.tokenize(seed.lower())).view(1, -1)\n",
        "    hidden = model.init_hidden(len(input))\n",
        "\n",
        "    continuation = ''\n",
        "    \n",
        "    for _ in range(num_tokens):\n",
        "        output, hidden = model(input, hidden)\n",
        "        \n",
        "        token_probas = output.squeeze().div(temperature).exp().cpu()\n",
        "        token = torch.multinomial(token_probas, 1)[0]\n",
        "\n",
        "        continuation += dataset.vocab.idx2char[token.item()]# допишите соответствующий символ\n",
        "\n",
        "        input = seed + continuation\n",
        "        input = torch.LongTensor(dataset.vocab.tokenize(input.lower())).view(1, -1) # обновите input\n",
        "    \n",
        "    return continuation"
      ],
      "metadata": {
        "id": "MEF2KtTiPKk-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "h6hDErpESl24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d1ab40-12b6-4866-f754-30b3f1617d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Странная ошибка, символа <, которого нигде нет\n",
            "\n",
            "Встретились англичанин, американец и русский. Англичанин говорит:... аыи    осс\n",
            "\n",
            "Так вот, однажды качки решили делать ремонт... р,ыые  оас\n",
            "\n",
            "Поручик Ржевский был... уеиеаоарар\n",
            "\n",
            "Идёт Будда с учениками по дороге... т  ззсмчрт\n",
            "\n",
            "Мюллер: Штирлиц, где вы были в 1930 году?... уееи  еыон\n",
            "\n",
            "Засылают к нам американцы шпиона под видом студента... ы ааргяуг \n",
            "\n",
            "Подъезжает электричка к Долгопе:... ррруаллеее\n",
            "\n"
          ]
        }
      ],
      "source": [
        "beginnings = ['Шел медведь по лесу', \n",
        "              'Встретились англичанин, американец и русский. Англичанин говорит:',\n",
        "              'Так вот, однажды качки решили делать ремонт',\n",
        "              'Поручик Ржевский был',\n",
        "              'Идёт Будда с учениками по дороге',\n",
        "              'Мюллер: Штирлиц, где вы были в 1930 году?',\n",
        "              'Засылают к нам американцы шпиона под видом студента',\n",
        "              'Подъезжает электричка к Долгопе:']\n",
        "\n",
        "for beginning in beginnings:\n",
        "    try:\n",
        "        print(f'{beginning}... {sample(10, beginning)}')\n",
        "        print()\n",
        "    except:\n",
        "        print('Странная ошибка, символа <, которого нигде нет\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#надо бы допилить . <eos>"
      ],
      "metadata": {
        "id": "n9epWydbRbi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "lm_pytorch.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}